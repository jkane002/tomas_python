{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lesson 4: Web Scraping",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uq6oR46pHrLH",
        "colab_type": "text"
      },
      "source": [
        "# Lesson 4: Web Scraping\n",
        "---\n",
        "Intro: \n",
        "Today we will learn about the benefits and applications of web scraping using Python.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QanMmuiXOoLE",
        "colab_type": "text"
      },
      "source": [
        "# Review\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAC89lCJcOa7",
        "colab_type": "text"
      },
      "source": [
        "1. What are sets?\n",
        "2. What kinds of operations can you use with sets?\n",
        "3. What's the difference between a set and a frozen set?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "glCedjN-Gd6s",
        "colab_type": "text"
      },
      "source": [
        "# Concept 1: Web Scraping\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25oLA8_cHOH7",
        "colab_type": "text"
      },
      "source": [
        "## What is it?\n",
        "Web scraping is the process of using bots to extract content and data from a website. It extracts underlying HTML code and, with it, data stored in a database. The scraper can then replicate entire website content elsewhere.\n",
        "\n",
        "Although this sounds and quite possibly is malicious, there are many benefits that outweigh negative perspectives. \n",
        "Web scraping is used in a variety of digital businesses that rely on data harvesting. Legitimate use cases include:\n",
        "\n",
        "* Search engine bots crawling a site, analyzing its content and then ranking it.\n",
        "* Price comparison sites deploying bots to auto-fetch prices and product descriptions for allied seller websites.\n",
        "* Market research companies using scrapers to pull data from forums and social media\n",
        "\n",
        "## Tools\n",
        "\n",
        "1. [BeautifulSoup4](https://www.crummy.com/software/BeautifulSoup/bs4/doc/) - a Python library for pulling data out of HTML and XML files. It's easy to learn and master. However, it requires dependencies. \n",
        "2. [Selenium](https://www.selenium.dev/documentation/en/) - Automated testing and web scraping at the same time. It's versatile as it works with javascript and HTML. However, not the best for only web scraping.\n",
        "3. [Scrapy](https://docs.scrapy.org/en/latest/) - Very fast and efficient but can be complex.\n",
        "\n",
        "We will be using BeautifulSoup4."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZBpqEKtG_oj",
        "colab_type": "text"
      },
      "source": [
        "## Examples:\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veX8ergwJh6W",
        "colab_type": "text"
      },
      "source": [
        "1. The Honey extension, searches other websites to find the best available price\n",
        "2. Data analytics, machine learning, data science\n",
        "3. Finance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lKe-QKCKHDHF",
        "colab_type": "text"
      },
      "source": [
        "## DIY:\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mGMqc6uCKKQ8",
        "colab_type": "text"
      },
      "source": [
        "Name other places where web scraping can be useful and harmful."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DTpiaBnQH4Wm"
      },
      "source": [
        "# Concept 2: HTML Basics\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wBM3DQ2hH4Wp"
      },
      "source": [
        "## Quick Lesson on HTML\n",
        "Hypertext Markup Language (HTML) is the standard markup language for documents designed to be displayed in a web browser.\n",
        "[HTML Graphic](https://images.app.goo.gl/Cd4r56rewkvjp4qRA)\n",
        "\n",
        "All HTML documents must start with a document type declaration: \\<!DOCTYPE html>.\n",
        "The HTML document itself begins with \\<html> and ends with \\</html>.\n",
        "The visible part of the HTML document is between \\<body> and \\</body>.\n",
        "```\n",
        "<!DOCTYPE html>\n",
        "<html>\n",
        "<body>\n",
        "<h1>My First Heading</h1>\n",
        "<p>My first paragraph.</p>\n",
        "</body>\n",
        "</html>\n",
        "```\n",
        "## HTML Heading Tag\n",
        "HTML headings are defined with the \\<h1> to \\<h6> tags.\n",
        "\\<h1> defines the most important heading. \\<h6> defines the least important heading: \n",
        "```\n",
        "<h1>This is heading 1</h1>\n",
        "<h2>This is heading 2</h2>\n",
        "<h3>This is heading 3</h3>\n",
        "```\n",
        "\n",
        "## HTML Paragraph Tag\n",
        "HTML paragraphs are defined with the \\<p> tag:\n",
        "```\n",
        "<p>This is a paragraph.</p>\n",
        "<p>This is another paragraph.</p>\n",
        "```\n",
        "\n",
        "## HTML Link Tag\n",
        "HTML links are defined with the \\<a> tag:\n",
        "```\n",
        "<a href=\"https://www.google.com\">This is a link</a>\n",
        "```\n",
        "The link's destination is specified in the href attribute. \n",
        "Attributes are used to provide additional information about HTML elements.\n",
        "\n",
        "## Unordered HTML List Tag\n",
        "An unordered list starts with the \\<ul> tag. Each list item starts with the \\<li> tag.\n",
        "\n",
        "The list items will be marked with bullets (small black circles) by default:\n",
        "```\n",
        "<ul>\n",
        "  <li>Coffee</li>\n",
        "  <li>Tea</li>\n",
        "  <li>Milk</li>\n",
        "</ul>\n",
        "```\n",
        "## Using The id Attribute\n",
        "The HTML id attribute specifies a unique id for an HTML element. The value of the id attribute must be unique within the HTML document.\n",
        "\n",
        "The id attribute is used to point to a specific style declaration in a style sheet. It is also used by JavaScript to access and manipulate the element with the specific id.\n",
        "\n",
        "```\n",
        "<h1 id=\"myHeader\">My Header</h1>\n",
        "```\n",
        "\n",
        "## HTML \\<div> Tag\n",
        "The \\<div> tag defines a division or a section in an HTML document.\n",
        "\n",
        "The \\<div> tag is used as a container for HTML elements - which is then styled with CSS or manipulated with JavaScript.\n",
        "\n",
        "The \\<div> tag is easily styled by using the class or id attribute.\n",
        "\n",
        "Any sort of content can be put inside the \\<div> tag!\n",
        "\n",
        "```\n",
        "<body>\n",
        "\n",
        "<div class=\"myDiv\">\n",
        "  <h2>This is a heading in a div element</h2>\n",
        "  <p>This is some text in a div element.</p>\n",
        "</div>\n",
        "\n",
        "</body>\n",
        "```\n",
        "\n",
        "## Bold Tag\n",
        "\n",
        "```\n",
        "<b>and this is bold text</b>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "FlP7DQBaH4Wq"
      },
      "source": [
        "## Examples:\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s-xOMQAlH4Wr",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130
        },
        "outputId": "868055c6-2468-443d-c512-6afd752c232d"
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html>\n",
        "  <body>\n",
        "    <h1>My First Heading</h1>\n",
        "    <p>My first paragraph.</p>\n",
        "  </body>\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-1-7c66002eac92>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <!DOCTYPE html>\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7wb0tz-g2tF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html>\n",
        "  <body>\n",
        "    <ul>\n",
        "      <li> Basketball </li>\n",
        "      <li> Soccer </li>\n",
        "      <li> Baseball </li>\n",
        "    </ul>\n",
        "  </body>\n",
        "</html>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JYdNesnGH4Ww"
      },
      "source": [
        "## DIY:\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kClvUWpJhG_K",
        "colab_type": "text"
      },
      "source": [
        "1. Create an unordered list of different names in HTML."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uQqN_Jq7H4Wx",
        "colab": {}
      },
      "source": [
        "<!DOCTYPE html>\n",
        "<html>\n",
        "  <body>\n",
        "    <ul>\n",
        "      <li> Tomas </li>\n",
        "      <li> Jonathan </li>\n",
        "      <li> Bob </li>\n",
        "    </ul>\n",
        "  </body>\n",
        "</html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fJqgEGhjH49C"
      },
      "source": [
        "# Concept 3: Setting up for 1st Program\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K_vkEh3zuIiX",
        "colab_type": "text"
      },
      "source": [
        "We will use the BeautifulSoup4 and requests modules. As mentioned before, BS4 will parse through the HTML code. With the requests module, you can send HTTP requests using Python. More simply, we can work with a specific website. BeautifulSoup4 allows you to read the website whereas requests allows you to choose which website to work on.\n",
        "\n",
        "## Installing Packages\n",
        "1. Open GitBash and type in:\n",
        "```\n",
        "python -m pip install -U pip\n",
        "```\n",
        "This not only checks if pip is installed but also upgrades pip to the most recent version. Pip is standard package-management system used to install and manage software packages written in Python. We need to install external packages (BS4 and requests) so we can use them. \n",
        "\n",
        "2. Next we need to install BeautifulSoup4. Type in:\n",
        "```\n",
        "pip install BeautifulSoup4\n",
        "```\n",
        "\n",
        "3. Now, to install requests:\n",
        "```\n",
        "pip install requests\n",
        "```\n",
        "\n",
        "Since we have both packages installed, let's look at them a bit closer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "oYEBZ37IH49G"
      },
      "source": [
        "## DIY:\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MpbOalEIt1Jc",
        "colab_type": "text"
      },
      "source": [
        "1. What is pip?\n",
        "2. How can we use these packages in our program? Hint: these are modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VBZdI5mMH5gq"
      },
      "source": [
        "# Concept 4: BeautifulSoup4 Documentation\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e2Y-bIUhH5gr"
      },
      "source": [
        "## Importing BeautifulSoup4\n",
        "\n",
        "```\n",
        "from bs4 import BeautifulSoup\n",
        "```\n",
        "Recall that we are importing a specific module from bs4. If this doesn't work for you, make sure you installed BeautifulSoup!\n",
        "\n",
        "## Constructor\n",
        "\n",
        "```\n",
        "soup = BeautifulSoup(mywebsite, \"html.parser\")\n",
        "```\n",
        "* soup is just an object name, common name is soup\n",
        "* 1st arg: The document you're going to parse - (analyze)\n",
        "* 2nd arg: The parser tool. For this instance, we will always use \"html.parser\" because we will parse through an html document. Python already supports the html.parser. \n",
        "\n",
        "## Kinds of Objects\n",
        "Beautiful Soup transforms a complex HTML document into a complex tree of Python objects. But you’ll only ever have to deal with about four kinds of objects: Tag, NavigableString, BeautifulSoup, and Comment.\n",
        "\n",
        "### Tag\n",
        "A Tag object corresponds to an XML or HTML tag in the original document:\n",
        "\n",
        "```\n",
        ">>> soup = BeautifulSoup('<b class=\"boldest\">Extremely bold</b>')\n",
        ">>> tag = soup.b\n",
        ">>> type(tag)\n",
        "# <class 'bs4.element.Tag'>\n",
        "```\n",
        "\n",
        "> Note: In the constructor you only see one argument. By default, it just reads the html document or string in this case using Python's default html.parser. For our examples, we will explicitly call \"html.parser\"\n",
        "\n",
        "### Name\n",
        "Every tag has a name, accessible as .name:\n",
        "```\n",
        "tag.name\n",
        "# u'b'\n",
        "```\n",
        "If you change a tag’s name, the change will be reflected in any HTML markup generated by Beautiful Soup:\n",
        "\n",
        "```\n",
        "tag.name = \"blockquote\"\n",
        "tag\n",
        "# <blockquote class=\"boldest\">Extremely bold</blockquote>\n",
        "```\n",
        "\n",
        "### Attributes\n",
        "A tag may have any number of attributes. The tag \\<b id=\"boldest\"> has an attribute “id” whose value is “boldest”. You can access a tag’s attributes by treating the tag like a dictionary:\n",
        "\n",
        "```\n",
        "tag['id']\n",
        "# u'boldest'\n",
        "```\n",
        "You can access that dictionary directly as .attrs:\n",
        "\n",
        "```\n",
        "tag.attrs\n",
        "# {u'id': 'boldest'}\n",
        "```\n",
        "\n",
        "### NavigableString\n",
        "A string corresponds to a bit of text within a tag. Beautiful Soup uses the NavigableString class to contain these bits of text:\n",
        "\n",
        "```\n",
        "tag.string\n",
        "# u'Extremely bold'\n",
        "type(tag.string)\n",
        "# <class 'bs4.element.NavigableString'>\n",
        "```\n",
        "\n",
        "### BeautifulSoup\n",
        "The BeautifulSoup object represents the parsed document as a whole. For most purposes, you can treat it as a Tag object. This means it supports most of the methods described in Navigating the tree and Searching the tree.\n",
        "\n",
        "### Comment\n",
        "The Comment object is just a special type of NavigableString:\n",
        "\n",
        "```\n",
        "comment\n",
        "# u'Hey, buddy. Want to buy a used parser'\n",
        "```\n",
        "But when it appears as part of an HTML document, a Comment is displayed with special formatting:\n",
        "\n",
        "```\n",
        "print(soup.b.prettify())\n",
        "# <b>\n",
        "#  <!--Hey, buddy. Want to buy a used parser?-->\n",
        "# </b>\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVPurrx2eK1t",
        "colab_type": "text"
      },
      "source": [
        "## Searching the tree\n",
        "Beautiful Soup defines a lot of methods for searching the parse tree, but they’re all very similar. I’m going to spend a lot of time explaining the two most popular methods: find() and find_all().\n",
        "\n",
        "Here's the example:\n",
        "```\n",
        "html_doc = \"\"\"\n",
        "<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        "\n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        "\n",
        "<p class=\"story\">...</p>\n",
        "\"\"\"\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "```\n",
        "\n",
        "### A String\n",
        "The simplest filter is a string. Pass a string to a search method and Beautiful Soup will perform a match against that exact string. This code finds all the \\<b> tags in the document:\n",
        "```\n",
        "soup.find_all('b')\n",
        "# [<b>The Dormouse's story</b>]\n",
        "```\n",
        "\n",
        "### find_all()\n",
        "The find_all() method looks through a tag’s descendants and retrieves all descendants that match your filters.\n",
        "```\n",
        "soup.find_all(\"title\")\n",
        "# [<title>The Dormouse's story</title>]\n",
        "\n",
        "soup.find_all(\"p\", \"title\")\n",
        "# [<p class=\"title\"><b>The Dormouse's story</b></p>]\n",
        "\n",
        "soup.find_all(\"a\")\n",
        "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
        "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
        "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]\n",
        "\n",
        "soup.find_all(id=\"link2\")\n",
        "# [<a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>]\n",
        "```\n",
        "\n",
        "### find()\n",
        "The find_all() method scans the entire document looking for results, but sometimes you only want to find one result. If you know a document only has one \\<body> tag, it’s a waste of time to scan the entire document looking for more. Rather than passing in limit=1 every time you call find_all, you can use the find() method. These two lines of code are nearly equivalent:\n",
        "\n",
        "```\n",
        "soup.find_all('title', limit=1)\n",
        "# [<title>The Dormouse's story</title>]\n",
        "\n",
        "soup.find('title')\n",
        "# <title>The Dormouse's story</title>\n",
        "```\n",
        "The only difference is that find_all() returns a list containing the single result, and find() just returns the result.\n",
        "\n",
        "If find_all() can’t find anything, it returns an empty list. If find() can’t find anything, it returns None:\n",
        "```\n",
        "print(soup.find(\"nosuchtag\"))\n",
        "# None\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gUoRk7q9H5gs"
      },
      "source": [
        "## Examples:\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAlS64mLrXbE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d42b9f77-7553-4fd0-db1f-1d0e21f273fb"
      },
      "source": [
        "html_doc = \"\"\"\n",
        "<html><head><title>The Dormouse's story</title></head>\n",
        "<body>\n",
        "<p class=\"title\"><b>The Dormouse's story</b></p>\n",
        " \n",
        "<p class=\"story\">Once upon a time there were three little sisters; and their names were\n",
        "<a href=\"http://example.com/elsie\" class=\"sister\" id=\"link1\">Elsie</a>,\n",
        "<a href=\"http://example.com/lacie\" class=\"sister\" id=\"link2\">Lacie</a> and\n",
        "<a href=\"http://example.com/tillie\" class=\"sister\" id=\"link3\">Tillie</a>;\n",
        "and they lived at the bottom of a well.</p>\n",
        " \n",
        "<p class=\"story\">...</p>\n",
        "\"\"\"\n",
        " \n",
        "from bs4 import BeautifulSoup\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "soup.find('p')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<p class=\"title\"><b>The Dormouse's story</b></p>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KC3fOHqGH5gs",
        "colab": {}
      },
      "source": [
        "soup.find_all('a')\n",
        "# [<a class=\"sister\" href=\"http://example.com/elsie\" id=\"link1\">Elsie</a>,\n",
        "#  <a class=\"sister\" href=\"http://example.com/lacie\" id=\"link2\">Lacie</a>,\n",
        "#  <a class=\"sister\" href=\"http://example.com/tillie\" id=\"link3\">Tillie</a>]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xw0HadAVp5qG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "soup.p\n",
        "# <p class=\"title\"><b>The Dormouse's story</b></p>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lt3_x0F5H5gv"
      },
      "source": [
        "## DIY:\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6N1fUC0-rNrP",
        "colab_type": "text"
      },
      "source": [
        "1. import BeautifulSoup\n",
        "2. Use this html_doc\n",
        "```\n",
        "html_doc = '<head> <title>The Dormouse\\'s story </title> </head>'\n",
        "```\n",
        "3. Create an instance of BeautifulSoup\n",
        "4. print out the title"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lwyGS48wH6Bj"
      },
      "source": [
        "# Concept 5: Requests Documentation\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "949yDmA_H6Bk"
      },
      "source": [
        "## Importing requests\n",
        "```\n",
        "import requests\n",
        "```\n",
        "Make sure to have requests installed!\n",
        "\n",
        "The requests module allows you to send HTTP requests using Python.\n",
        "\n",
        "The HTTP request returns a Response Object with all the response data (content, encoding, status, etc). Just know you're able to retrieve website information.\n",
        "\n",
        "General syntax:\n",
        "```\n",
        "requests.methodname(params)\n",
        "```\n",
        "\n",
        "Method\tDescription\n",
        "\n",
        "We will use:\n",
        "* get(url, params, args)\tSends a GET request to the specified url\n",
        "\n",
        "Here are others:\n",
        "* delete(url, args)\tSends a DELETE request to the specified url\n",
        "* head(url, args)\tSends a HEAD request to the specified url\n",
        "* patch(url, data, args)\tSends a PATCH request to the specified url\n",
        "* post(url, data, json, args)\tSends a POST request to the specified url\n",
        "* put(url, data, args)\tSends a PUT request to the specified url\n",
        "* request(method, url, args)\tSends a request of the specified method to the specified url\n",
        "\n",
        "Since the HTTP request returns a response object, we then use:\n",
        "* content - Returns the content of the response, in bytes\n",
        "\n",
        "Don't worry if you don't understand, it'll come together when we work with these methods."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "An0-NOPcH6Bl"
      },
      "source": [
        "## Example:\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "_6YebfJHH6Bl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        },
        "outputId": "05bb4a32-46e4-4251-9bee-d751d9b2cd68"
      },
      "source": [
        "import requests\n",
        "\n",
        "x = requests.get('https://w3schools.com/python/demopage.htm')\n",
        "\n",
        "print(x.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<!DOCTYPE html>\r\n",
            "<html>\r\n",
            "<body>\r\n",
            "\r\n",
            "<h1>This is a Test Page</h1>\r\n",
            "\r\n",
            "</body>\r\n",
            "</html>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gPTKoKucH6Bn"
      },
      "source": [
        "## DIY:\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZQASKrYuvuA",
        "colab_type": "text"
      },
      "source": [
        "1. import requests\n",
        "2. Use get to retrieve the data from https://en.wikipedia.org/wiki/LeBron_James\n",
        "3. Print it out"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o26AWkwAH6Bo",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "wiki =  requests.get(' https://en.wikipedia.org/wiki/LeBron_James')\n",
        "\n",
        "print(wiki.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mneCw_ASH6pK"
      },
      "source": [
        "# Concept 6: Putting it all together\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "KpheOMOAH6pL"
      },
      "source": [
        "## Outline\n",
        "We will go to Wikipedia and extract the titles and links of the In The News Section.\n",
        "\n",
        "1. Let's analyze the webpage first. Go to https://en.wikipedia.org/wiki/Main_Page.\n",
        "2. We can check the html code using the inspector by pressing cntrl + option + i or rightclick->inspect.\n",
        "3. Hover over the In The News section.\n",
        "4. You should see a \\<div> containing the section with an id of \"mp-itn\"\n",
        "5. Click on the \\<div> and find the \\<ul> unordered list with \\<li> elements. Inside you should see \\<a> tags for hyperlinks. The href id is the link to that website.\n",
        "6. Now let's code!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wVXpvsjfW7PP",
        "colab_type": "text"
      },
      "source": [
        "## Code on the Fly / DIY\n",
        "1. import requests and from bs4 import BeautifulSoup. Remember using BeautifulSoup can analyze the data and using requests can request the specific website you need.\n",
        "```\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "```\n",
        "2. Assign a variable called url with the url as a string. \n",
        "```\n",
        "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
        "```\n",
        "3. Create a variable called response. Use requests' get method and pass in the url. Remember that get sends a GET response from the url provided.\n",
        "```\n",
        "response = requests.get(url)\n",
        "```\n",
        "4. Create another variable called page and assign it with response's content attribute. This returns the content in bytes.\n",
        "```\n",
        "page = response.content\n",
        "```\n",
        "5. Now let's make the soup. Call BeautifulSoup's constructor. 1st argument is page and the 2nd argument is 'html.parser' Again this tells BeautifulSoup that we will use the html parser to analyze an html document.\n",
        "```\n",
        "soup = BeautifulSoup(page, 'html.parser')\n",
        "```\n",
        "6. Create a variable called inTheNews and use BeautifulSoup's find method. 1st parameter is 'div' and 2nd argument is id='mp-itn'. This finds the In The News section in Wikipedia. \n",
        "```\n",
        "inTheNews = soup.find('div', id='mp-itn')\n",
        "```\n",
        "7. Have another variable called news that uses the variable inTheNews to find all lists. Remember to use the find_all() method. This finds each individual news.\n",
        "```\n",
        "news = inTheNews.find_all('li')\n",
        "```\n",
        "8. Now let's loop through the news. \n",
        "```\n",
        "for n in news:\n",
        "```\n",
        "9. For each element in the news, we want to print the headline. We can use the get_text() method for this. This prints out the title of the news in text format.\n",
        "```\n",
        "print(n.get_text())\n",
        "```\n",
        "10. Now that we can iterate through each list, let's find all tags that have links. Remember that we also want to find the links to each news. The a tag is for hyperlinks.\n",
        "```\n",
        "links = n.find_all('a')\n",
        "```\n",
        "11. Let's loop through the links. For each element we will print out the website link. This for loop is nested.\n",
        "```\n",
        "for link in links:\n",
        "  print(link['href'])\n",
        "```\n",
        "12. Outside the second for loop, print a new line so we can get a clean output."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q7uCzxxmGzqP",
        "colab_type": "text"
      },
      "source": [
        "# Summary:\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poW0S8eJtlxq",
        "colab_type": "text"
      },
      "source": [
        "1. What is web scraping?\n",
        "2. What is BeautifulSoup?\n",
        "3. How can web scraping be used in real life?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4AmHQnAO6UP9",
        "colab_type": "text"
      },
      "source": [
        "# Homework:\n",
        "---\n",
        "1. Read more about HTML [via the html glossary](https://www.codecademy.com/articles/glossary-html)\n",
        "1. You are given two sets. Set A= 1,2,3,4,5,6, Set B= 2,3,4,5,6,7,8.\n",
        "How many elements are present in A union B? A intersection B?\n",
        "2. Get input from the user. Ask for their name. Then ask for the year they were born. Check to see if the year they were born in is either odd or even. Should look like this:\n",
        "```\n",
        "Sarah, you were born in 1989. \n",
        "That is an odd year.\n",
        "```\n",
        "3. Similar to the web scraping diy you did before, ask the user to enter a website. Now find all links from that website.\n",
        "Example:\n",
        "```\n",
        "Enter a website to extract the URL's from: google.com\n",
        "```\n",
        "The output is this:\n",
        "```\n",
        "http://www.google.com/imghp?hl=en&tab=wi\n",
        "http://maps.google.com/maps?hl=en&tab=wl\n",
        "https://play.google.com/?hl=en&tab=w8\n",
        "http://www.youtube.com/?gl=US&tab=w1\n",
        "http://news.google.com/nwshp?hl=en&tab=wn\n",
        "https://mail.google.com/mail/?tab=wm\n",
        "https://drive.google.com/?tab=wo\n",
        "https://www.google.com/intl/en/about/products?tab=wh\n",
        "http://www.google.com/history/optout?hl=en\n",
        "/preferences?hl=en\n",
        "https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=http://www.google.com/\n",
        "/advanced_search?hl=en&authuser=0\n",
        "/intl/en/ads/\n",
        "/services/\n",
        "/intl/en/about.html\n",
        "/intl/en/policies/privacy/\n",
        "/intl/en/policies/terms/\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sGIYpA2949ZK",
        "colab_type": "text"
      },
      "source": [
        "# Notes on homework:\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eyWHUG-WtVRj",
        "colab_type": "text"
      },
      "source": [
        "I will check in on Thursday,  through email to check on your progress. Respond with any questions you might have. Otherwise, a simple “all good” is appropriate if you have no questions or comments. \n",
        "\n",
        "You will need to upload your coding homework assignments to GitHub.\n",
        "1. In gitbash, change directories to the homework directory: tomas_python/homework\n",
        "* TIP: use ‘cd’ to change directories\n",
        "* Use ‘cd ..’ to return to the previous directory\n",
        "* Use ‘pwd’ to show full pathname of the current working directory \n",
        "* Use ‘ls’ to list all your directories\n",
        "2. Once you’re in that directory, type in ‘git pull’\n",
        "* This ensures you have all updated files\n",
        "* If there is an error involved, email me immediately so we can try resolving it.\n",
        "* Otherwise, type your code below and we’ll resolve issues next class\n",
        "3. To create a new file, type in ‘touch hw01.py’ or the appropriate file name\n",
        "* ‘Touch’ creates a new file\n",
        "4. Open up the python file and start coding!\n",
        "\n",
        "Note: Become familiar with these actions. This is essentially what happens in the backend when you right-click and create a new folder/file!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddKcgLGshg1S",
        "colab_type": "text"
      },
      "source": [
        "# DIY Solutions\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gRSIvIhdhjzs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "<ul>\n",
        "  <li> Tomas </li>\n",
        "  <li> Sarah </li>\n",
        "  <li> Bobby </li>\n",
        "</ul>"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AculvS6IH5gv",
        "colab": {}
      },
      "source": [
        "from bs4 import BeautifulSoup\n",
        "\n",
        "html_doc = '<head> <title>The Dormouse\\'s story </title> </head>'\n",
        "soup = BeautifulSoup(html_doc, 'html.parser')\n",
        "\n",
        "soup.title\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0n7_Od6u9dW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import requests\n",
        "\n",
        "x = requests.get('https://en.wikipedia.org/wiki/LeBron_James')\n",
        "\n",
        "print(x.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MqL9nG3xkA3E",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "febece8f-b9dc-45ad-ea4e-af3958b52907"
      },
      "source": [
        "# 1. Get page data\n",
        "import requests\n",
        "\n",
        "url = 'https://en.wikipedia.org/wiki/Main_Page'\n",
        "response = requests.get(url)\n",
        "\n",
        "page = response.content\n",
        "\n",
        "# 2. Work with page data\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "soup = BeautifulSoup(page, 'html.parser')\n",
        "\n",
        "inTheNews = soup.find('div', id='mp-itn')\n",
        "\n",
        "news = inTheNews.find_all('li')\n",
        "\n",
        "for n in news:\n",
        "    print(n.get_text())\n",
        "    links = n.find_all('a')\n",
        "    for link in links:\n",
        "        print(link['href'])\n",
        "    print('\\t')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Disease\n",
            "/wiki/Coronavirus_disease_2019\n",
            "\t\n",
            "Virus\n",
            "/wiki/Severe_acute_respiratory_syndrome_coronavirus_2\n",
            "\t\n",
            "Testing\n",
            "/wiki/COVID-19_testing\n",
            "\t\n",
            "Timeline\n",
            "/wiki/Timeline_of_the_COVID-19_pandemic\n",
            "\t\n",
            "By location\n",
            "/wiki/COVID-19_pandemic_by_country_and_territory\n",
            "\t\n",
            "Impact\n",
            "/wiki/Impact_of_the_COVID-19_pandemic\n",
            "\t\n",
            "Notable deaths\n",
            "/wiki/List_of_deaths_due_to_COVID-19\n",
            "\t\n",
            "Portal\n",
            "/wiki/Portal:Coronavirus_disease_2019\n",
            "\t\n",
            "Comet NEOWISE (pictured) is visible to the naked eye in the Northern Hemisphere.\n",
            "/wiki/C/2020_F3_(NEOWISE)\n",
            "/wiki/Naked_eye#In_astronomy\n",
            "\t\n",
            "In the Singaporean general election, Lee Hsien Loong is re-elected Prime Minister as his People's Action Party retains its supermajority.\n",
            "/wiki/2020_Singaporean_general_election\n",
            "/wiki/Lee_Hsien_Loong\n",
            "/wiki/Prime_Minister_of_Singapore\n",
            "/wiki/People%27s_Action_Party\n",
            "\t\n",
            "Bulgaria and Croatia join the European Exchange Rate Mechanism 2, the first major step in their adoption of the euro.\n",
            "/wiki/European_Exchange_Rate_Mechanism\n",
            "/wiki/Euro\n",
            "\t\n",
            "A bus plunges into a reservoir in Anshun, China, killing 21 people and injuring 16.\n",
            "/wiki/Anshun_bus_crash\n",
            "/wiki/Anshun\n",
            "\t\n",
            "George Floyd protests\n",
            "/wiki/George_Floyd_protests\n",
            "\t\n",
            "Zindzi Mandela\n",
            "/wiki/Zindzi_Mandela\n",
            "\t\n",
            "Jyotsna Bhatt\n",
            "/wiki/Jyotsna_Bhatt\n",
            "\t\n",
            "Gabriella Tucci\n",
            "/wiki/Gabriella_Tucci\n",
            "\t\n",
            "Frank Bolling\n",
            "/wiki/Frank_Bolling\n",
            "\t\n",
            "Jack Charlton\n",
            "/wiki/Jack_Charlton\n",
            "\t\n",
            "Lara van Ruijven\n",
            "/wiki/Lara_van_Ruijven\n",
            "\t\n",
            "Nominate an article\n",
            "/wiki/Wikipedia:In_the_news/Candidates\n",
            "\t\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}